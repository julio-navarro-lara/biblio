Computer Communications 35 (2012) 1368–1379

Contents lists available at SciVerse ScienceDirect

Computer Communications
journal homepage: www.elsevier.com/locate/comcom

Multi-layer episode ﬁltering for the multi-step attack detection
Mahbobeh Soleimani a,⇑, Ali A. Ghorbani a
a

Information Security Centre of Excellence, Faculty of Computer Science, University of New Brunswick, Fredericton, New Brunswick, Canada

a r t i c l e

i n f o

Article history:
Received 2 November 2009
Received in revised form 2 April 2012
Accepted 2 April 2012
Available online 18 April 2012
Keywords:
Alert correlation
Multi-step attack
Intrusion detection system

a b s t r a c t
The discovery of sophisticated attack sequences demands the development of signiﬁcantly better alert
correlation algorithms. Most of the proposed approaches in the area of multi-step attack detection have
limited capabilities because they rely on various forms of predeﬁned knowledge of attacks or attack transition patterns using attack modeling language or pre-and post-conditions of individual attacks. Therefore, those approaches cannot recognize a correlation when an attack is new or the relationship
between attacks is new. In this research, we take a different view and consider alert correlation as the
problem of inferring an intruder’s actions as alert patterns that are constructed progressively. The work
is based on a multi-layer episode mining and ﬁltering algorithm. A decision-tree-based method is used
for learning speciﬁcations of each attack pattern and detecting them in alert streams. We also used a Correlation Weight Matrix (CWM) for encoding correlation strength between attack types in the attack scenarios. One of the distinguishing features of our proposed technique is detecting novel multi-step attack
scenarios, using a rule prediction method. The results have shown that our approach can effectively discover known and unknown attack strategies with high accuracy. We achieved more than 90% reduction
in the number of discovered patterns while more than 95% of ﬁnal patterns were actual patterns. Furthermore, our rule prediction capability showed a precise forecasting ability in guessing future alerts.
Ó 2012 Elsevier B.V. All rights reserved.

1. Introduction
Intrusion Detection Systems (IDSs) trigger too many alerts that
usually contain false alerts that show either normal trafﬁc or failed
intrusion attempts. To make things worse, only 1% of the enormous
amount of alerts generated by most IDSs corresponds to unique
attacks [21].
Alert management and correlation improve the accuracy of IDS
signiﬁcantly. Decreasing false positives and improving the knowledge about attacks provides a more global view of what is happening in a network. Correlating alerts only based on the similarities
between their attributes cannot discover the real reasons why
alerts are correlated. On the other hand, considering previously
speciﬁed scenarios to perform the correlation is limited to discovering only known scenarios and a great hand work is needed to
specify each of them. It is also important to correlate alerts in real
time in order to have a more effective response time.
However, there are some approaches that try to ﬁnd the correlation between alerts based on the frequency of patterns in the
alert streams. Despite the fact that some types of multi-step attack
scenarios have a frequent behaviour, leaning on frequent patterns
can result in missing those scenarios that do not occur frequently

⇑ Corresponding author. Tel.: +1 604 374 8423.
E-mail address: m.soleimani@unb.ca (M. Soleimani).
0140-3664/$ - see front matter Ó 2012 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.comcom.2012.04.001

but are critical. Moreover, a large number of discovered frequent
patterns still need to be inspected by administrators manually in
order to ﬁnd the meaningful and critical ones.
In this work, we used an unsupervised pattern generation
method that can create all of the possible combinations of alerts
in an alert stream. In each step of the pattern generation method,
we applied a machine learning algorithm to ﬁlter those patterns
that follow speciﬁcations of a multi-step attack scenario. The pattern generation algorithm applies ﬂexible sliding windows for
detecting steps of attacks. Hence, steps of attacks can occur consequently with a pause between each pair of steps. Behaviour of
attackers in performing steps of attacks is learned gradually to consider a reasonable duration to complete each step.
Besides, an attack tree data structure is deﬁned for storing the
multi-step attacks that are discovered. In this tree, information
about the frequency of each step is retained. For discerning critical
attack patterns and false positives, we suggested a new learningbased episode reduction method. In the process of learning,
speciﬁcation of episodes rather than alerts is used for learning multi-step attack scenarios.
A Correlation Weight Matrix (CWM) is also used for encoding
correlation strength between attack types in the attack scenarios.
The attack tree data structure keeps information about the frequency of each step. As attack tree gets updated, rules that predict
forthcoming attacks get generated. Each rule’s degree of conﬁdence
is calculated based on a normalized distance deviation function

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

that measures the degree of similarity between a discovered attack
pattern and a known multi-step attack. Rules are used in the attack
prevention phase to guess the next step of an attack.
During each step of pattern ﬁltering, a rule-based classiﬁcation
method decides about each discarded pattern. If steps of a discarded pattern have strong correlation, discovery of a new attack
scenario can be inferred. If the correlation between steps of the
pattern is frail, the pattern is probably a false one. Novel discovered
attack scenarios will be used to update the CWM and learning
algorithm.
The results of the study have shown that our approach can
effectively discover known and unknown attack strategies with
high accuracy. We achieved more than 90% reduction in the number of discovered patterns while more than 95% of ﬁnal patterns
were actual patterns. Furthermore, our rule prediction capability
showed a precise forecasting ability in guessing future alerts.
The rest of the paper is organized as follows. In Section 2, some
of the related works in the area of alert correlation are reviewed.
Section 3 provides the details of the proposed framework for mining multi-step patterns in alert streams. Section 4 reports experiments that are conducted for evaluating the proposed technique.
Finally, the conclusions and some suggestions for future work are
given in Section 5.

2. Related works
Because IDSs usually trigger high volume of alerts, causality
analysis is essential in providing a higher level view of the actual
attacks. The causal relationships between alerts might represent
how they are related to each other in an attack scenario. Viinikka
et al. [17] aggregated individual alerts to alert ﬂows, and then processed the ﬂows instead of individual alerts to ﬁnd the correlation
between alerts for coping with the large quantity of alerts. Some of
the previous works in this category have used formal models for
specifying attack scenarios, like LAMBDA [1], STATL [2], ADeLe
[3], [5,9,10]. However, some correlation research works are based
on pre-deﬁned attack scenario rules [4,7,8,23].
Valuable research work has been done in the area of attack
strategy detection that mostly uses powerful machine learning
methods to ﬁnd multi-step attack scenarios. For example, in [6],
two machine learning techniques (Multilayer Perception and Support Vector Machine) have been used to estimate the correlation
probability between alerts. Moreover, Goldman et al. [7] built a correlation system based on Bayesian reasoning, and [20,16] used an
unsupervised learning method for network alert correlation.
The idea of Correlation Weight Matrix (CWM) which is developed in this paper is similar to ACM which is deﬁned in [6]. However, there is a difference between these two matrixes. The CWM
gets dynamically updated during both training and test phases.
But, in ACM correlation weights are incrementally updated during
training process.
Developing a real-time alert correlation method is one of the
goals of this research. There are some other research work that
used different methods for real-time alert correlation. For example,
Giannella et al.’s idea about mining time-sensitive data streams is
used in [11] for real-time alert correlation and attack strategy
detection. Core of the proposed framework is a new algorithm
(FSP Growth) for mining frequent patterns of alerts considering
their structures. Moreover, Wang et al. [19] developed an algorithm to correlate isolated alerts into the attack scenarios efﬁciently in terms of memory usage. Besides, in [12], a real-time
alert correlation system is proposed to detect an ongoing attack
and predict the upcoming next step of a multi-step attack in a
timely manner. In [15] an attack graph analysis method which
aggregates attack paths according to underlying network regulari-

1369

ties is used to prioritize IDS alerts and reduce the complexity of
analysis.
In this paper, we took advantage of episode generation method
and Decision Tree (DT) algorithm. The frequent episode and
episode rule mining problem was ﬁrst introduced by Mannila
[13]. Given a sequence of events, episode mining aims to ﬁnd all
of the episodes with occurrence frequencies satisfying the userspeciﬁed minimum support. However, almost all of them are based
on the Apriori property which states that any super-pattern of a
non-frequent pattern cannot be frequent. Most of apriori-like algorithms generate, potentially, huge sets of candidate episodes, and
require as many full sequence scans as the longest episode.
Telecommunication network Alarm Sequence Analyzer (TASA)
[14] applied the frequent episode mining algorithm in developing
an interactive knowledge discovery system for the events triggered
by network devices. TASA discovers frequent episode rules from
alarm streams in an off-line mode. Actually, the administrator is
able to see its favourite episodes by specifying some thresholds
and templates. Therefore, the only criteria that is applied for controlling number of generated episodes and ﬁltering the most
important ones is support and conﬁdence thresholds that both
are based on the frequency of occurrence. However, there are some
critical episodes that are not frequent and cannot be detected by
this approach. We will address this problem in our work using a
multi-layer episode veriﬁcation approach.
In [12,14,18], patterns or rules are extracted from a large number of alerts. In these series of studies, they paid less attention to
the number of patterns and verifying their criticality. While, an
applicable and real-time alert correlation system needs an efﬁcient
algorithm for discovering patterns and verifying their criticality.
3. Framework
The system is composed of two modes. It ﬁrst works in off-line
mode and then it starts working in on-line mode. In the off-line
mode, as is shown in Fig. 1, system receives a stream of training
alerts that are labelled. First, labelled alerts get aggregated and
then episode generation component uses them to generate different lengths of episodes. Next, learning component learns real multi-step attacks and false combination of alerts by generating
decision trees. In the on-line mode, as is illustrated in Fig. 2, system
continuously receives the on-line stream of alerts. First, alert
aggregation component aggregates the alerts and sends them to
the episode generation component. Second, episode generation
component generates different lengths of episodes. Up to here,
off-line and on-line modes work exactly alike. Next, episode generation component feeds the episodes into the detection component
to ﬁlter them based on their degree of criticality, which is learned
in off-line mode by learning component. Filtered critical episodes
go to the prediction component to predict future steps of attack.
Moreover, ﬁltered uncritical episodes go to the new strategy detec-

Fig. 1. Framework in the off-line mode.

1370

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

Fig. 2. Framework in the on-line mode.

tion component in order to detect new attack strategies and update
decision trees consequently.
4. Alert aggregation
Each IDS generated security alert consists of several parameters
that refer to the attributes of corresponding network trafﬁc that
matched the speciﬁcations of malicious activity. The most important parameters of an IDS triggered alerts are source IP address
of attacker, destination IP address of victim machine, source and
destination port number used to run the attack, attack type, attack
severity and time of occurrence. Attack type is the name assigned
to the malicious activity according to the IDS signature database. In
this work, source IP address, destination IP address, source and
destination port numbers, attack type, attack severity and time of
occurrence are the alert parameters used for correlating alerts.
Alert aggregation component combines all of the alerts that
have identical attack types. That is, redundant alerts will be shrunk
into more synthetic hyper alerts that their number is by far less
than original alerts. If we have a set of alerts fA ¼ At1 ; At2 ; . . . ; Atn g
in time interval t and each alert consists of a set of features that
are aforementioned, we call them similar if At1:attackType ¼
At2:attackType ¼    ¼ Atn:attackType . We deﬁne a hyper alert as an inclusive alert that represents all of similar alerts in an interval t and
is denoted as Ah where

Ah:srcIP ¼ fAt1:srcIP ; At2:srcIP ;    ; Atn:srcIP g;
Ah:dstIP ¼ fAt1:dstIP ; At2:dstIP ;    ; Atn:dstIP g;
Ah:srcPort ¼ fAt1:srcPort ; At2:srcPort ;    ; Atn:srcPort g;
Ah:dstPort ¼ fAt1:dstPort ; At2:dstPort ;    ; Atn:dstPort g;
Ah:t ¼ fAt1:time ; At2:time ;    ; Atn:time g;
Ah:attackType ¼ At1:attackType ;

win; Wðs; winÞ represents the set of all episode windows w on s
such that widthðwÞ ¼ win.
As deﬁned in [13], an episode is a partially ordered collection of
events occurring together that can be described as a directed acyclic graph. Episodes are deﬁned formally as a triple a ¼ ðV; 6; gÞ,
where V is a set of nodes, 6 is a partial order on V, and g : V ! A
is a mapping that associates each node with an alert. The interpretation of an episode is that the events in g ðVÞ have to occur in the
order described by 6. We deﬁne an event as an alert that is generated by an IDS. Therefore, g associates each node with an alert.
Consequently, based on the deﬁnitions in [13], serial episodes are
deﬁned formally as follows: Episode e is a serial episode if the relation 6 is a total order (i.e., x 6 y or y 6 x for all x, y 2 V). Concept of
serial episodes is used to extract different serial combinations of an
alert sequence to identify a critical sequence of alerts, i.e., multistep attack scenario.
Example. Consider episode e ¼ ðV; 6; gÞ in Fig. 4. The set V
contains two nodes; we denote them by x and y. The mapping g
labels these nodes with the alerts that are seen in the ﬁgure: g(x) =
Sadmind_Ping and g(y) = Sadmind_Amslverify_Overﬂow. In this
example, an alert of type Sadmind_Ping is supposed to occur before
an alert of type Sadmind_Amslverify _Overﬂow, i.e., x precedes y, and
we have x 6 y.
 6;
 gÞ is a subepisode of an episode
An episode e2 ¼ ðV;
e1 ¼ ðV; 6; gÞ, denoted e2  e1 , if there exist an injective mapping
 ! V such that gðv Þ ¼ gðf ðv ÞÞ for all v 2 V,
 and for all
f :V
 also f ðv Þ 6 f ðwÞ. An episode e1 is a super-epiv ; w 2 V with v 6w
sode of e2 if and only if e2  e1 . We write e2  e1 if e2  e1 and
:ðe1  e2 Þ.
Example. From Fig. 3 it is seen that e2  e3 , since e2 is a subgraph
of e3 . In terms of the deﬁnition, there is a mapping f that connects

Ah:attackSev erity ¼ At1:attackSev erity ;
Ah:counter ¼ number of similar alerts:
5. Episode generation
A sequence of alerts, s, is deﬁned by a triple, ðA; T s ; T e Þ where
A ¼ ðAt1 ; At2 ; . . . ; Atn Þ is an ordered sequence of alerts such that
ti 6 t iþ1 for all i ¼ 1; . . . ; n  1 and T s and T e are the start and the
end time of a sequence of alerts, where T s 6 t i < T e for all
i ¼ 1; . . . ; n.
Formally, an episode window on an alert sequence s ¼ ðA; T s ; T e Þ
is an alert sequence w ¼ ðw; ts ; t e Þ, where t s < T e and te > T s , and w
consists of those alerts At from s where ts 6 At:time < te . The time
span t e  ts is called the width of the window w, and it is denoted
by width(w). Given an alert sequence s and an integer

Fig. 3. (a) Episode e2 (b) Episode e3 .

Fig. 4. An example of a serial episode e.

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

1371

by Toivonen and Mannila [13] to be suitable for candidate episode
generation without considering their frequency. In this algorithm,
an episode e is represented as an array of alerts. The array is denoted by the name of the episode. For example, a serial episode e
with alerts of types A, C, C, and F is represented as an array e with
e½1 ¼ A; e½2 ¼ C; e½3 ¼ C, and e½4 ¼ F.
Algorithm 5.1: Candidate Critical Episode Generator ðF; lÞ
Fig. 5. Generating candidate attack sequences using window time of win.

the nodes labelled A with each other and the nodes labelled B with
each other, i.e., both nodes in episode e2 have corresponding nodes
in e3 . Since the nodes in episode e2 are ordered, the corresponding
nodes in e3 also need to be ordered.
We also deﬁne an episode tree ET ¼ ðE; ; FÞ as a set of episodes
E, sub-episode relation  and counters F for the number of occurrences of each node in the tree. At the beginning, each episode tree
has one null node which is called root node. Sub-episode relation is
used for ﬁnding the sub-episode relation between episodes to
avoid redundant episode insertion in the episode tree. For example,
Fig. 6(a) shows episodes e1 ; e2 ; e3 , and e4 that are attack episodes
and should be inserted into the episode tree. Episode e1 is a subepisode of episode e2 . As is shown in Fig. 6(b), super set e2 is inserted into root of the episode tree. After inserting the episodes
e3 and e4 , as shown in Fig. 6(c), the counter of those nodes that
are common with the episode e3 will be increased. These counters,
beside the occurrence window time of episodes, represent the frequency of each node in the episode tree, and frequency is used to
predict episode rules.
Before generating episodes, specially during real-time alert correlation, alerts need to be aggregated in batches. We deﬁne a batch
window as a period of time that IDS alerts are provided to the alert
aggregation component to reduce the number of alerts by combining similar ones. As is shown in Fig. 5, a batch window is a slice of a
sequence of alerts and the episode windows are sequences of partially overlapping windows.
Serial episodes can be constructed by sliding a batch window
and an episode window over a sequence of alerts as is shown in
Fig. 5. In each batch window, all of the alerts are aggregated, and
in each episode window, Algorithm 5.1 is used to discover all of
the possible episodes with a predeﬁned maximum length.
Dividing the alert sequence according to time window size,
which is shown in Fig. 5, leads to multiple candidate sequence
generations. Algorithm 5.1 generates candidate episodes that can
be either attack or benign episodes. Detection component, which
is discussed in detail in Section 7, uses candidate episodes and discovers attack and benign episodes. Algorithm 5.1 is a candidate
episode generation method. We adapt the method that was used

F: An array F l of serial episodes of size l
l: Length of episode
C: Candidate episode
current_block_start: Index of the ﬁrst episode in a block that
all of its block members of size l share l-1 attack types
k: Index of current_block_start for generated candidate
episode
Output: An array F lþ1 of serial episodes of size l þ 1
Initialization
for i
1 to l þ 1
do C½i ¼ /;
k = 0;
for i
1 to jF l j

8
current block start ¼ k þ 1;
>
>
>
>
>
for j ¼ i; F l :block start½j ¼ F l :block start½i; j ¼ j þ 1
>
>
> 8
>
>
>
> if j ¼ i then continue
>
> >
>
>
>
>
>
>
>
>
> with the next j;
>
> >
>
>
>
>
>
>
> comment : F l ½i and F l ½j have l  1 first event types
>
> >
>
>
>
>
in common; build a potential candidate e as their
< >
>
>
>
>
do
>
< combination :
>
>
>
do
>
for8x
1 to l
>
>
>
>
>
>
>
e½x
¼ F l ½i½x;
>
>
>
> >
> <
>
>
>
>
>
k ¼ k þ 1;
do
e½l
þ
1 ¼ F l ½j½l;
>
>
> >
> >
>
>
> >
> : store e as a candidate :
>
>
>
>
>
>
>
> >
> C lþ1 ½k ¼ e;
>
>
>
>
>
: >
:
C lþ1 :block start½k ¼ current block start;
outputC lþ1 ;

Collections of episodes are also represented as arrays, i.e., the
ith episode of a collectionF is denoted by F l ½i. Since the episodes
and episode collections are in order, all episodes that share the
same ﬁrst event types are consecutive in the episode collection.
In particular, if episodes F l ½i and F l ½j of size l share the ﬁrst l  1
events, then, for all k; i 6 k 6 j; F l ½k also shares the same events.
A maximal sequence of consecutive episodes of size l that share

Fig. 6. (a) Episodes e1 ; e2 ; e3 , and e4 . (b) Insertion of episodes e1 and e2 into episode tree. (c) Insertion of episodes e1 ; e2 ; e3 , and e4 into episode tree.

1372

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

Fig. 7. Learning component.

the ﬁrst l  1 events is called a block. Potential candidates can be
identiﬁed by creating all combinations of two episodes in the same
block. For the efﬁcient identiﬁcation of blocks, we store in
F l :block start½j for each episode F l ½j the i such that F l ½i is the ﬁrst
episode in the block.
2
The total time complexity of Algorithm 5.1 is OðlF l Þ.
Different episodes with different lengths can be constructed, as
is shown in Algorithm 5.1. After ﬁnding episodes, they are ﬁltered
based on their similarity to a multi-step attack scenario. Learning
and detecting multi-step attack scenarios is discussed in Sections
6 and 7, respectively. Episodes that are distinguished as a multistep attack or a subset of such an attack will be inserted into an
episode tree that is called a ‘‘model episode tree’’. A model episode
tree is shown in Fig. 6. In this ﬁgure, episodes AB, ABC, ADF and
ADGH are inserted into the episode tree. At the beginning superepisodes are detected. Since we are looking for multi-step attack
scenarios, windows for ﬁnding different lengths of episodes should
be ﬂexible enough for discovering all of the possible combinations
of alerts. For example, we consider that episode AB that is the
beginning part of a multi-step scenario ABCD is encountered in
window w1 . In the next window, w2 , episode AB takes part in the
subsequent episode generation process. This process will give an
incomplete episode the chance to be completed. However, there
is a threshold l for the number of times that an incomplete episode
is given the opportunity to become a complete episode.
In [13], for controlling the number of episodes a minimum frequency for each episode is used. That is, whenever in a given time
window an episode is extracted from a sequence of events, its
number of occurrences would be taken into the consideration. If
it is less that a minimum threshold, then it will be ignored.
Since we are looking for multi-step attacks or single-step attacks that are critical for a particular network, entire episodes
rather than only the frequent ones need to be considered. However, improper management of the extracted episodes causes generation of a large number of different length of episodes. Therefore,
we devised a new approach that handles the extracted episodes by
selecting the most critical ones. In this work, we rely on the expert
knowledge about the known critical single-step or multi-step
attack scenarios based on the network assets proﬁle. However, in
the other part of this work, we can detect new attack strategies
using rule-based classiﬁcation methods.
Imperfect detection of IDS that causes missing episodes is not
addressed in this paper. The most important characteristic of this
work is discerning the important patterns that are not recognizable
among too many raw events.

developing following components: Episode generation and labeling, Correlation Weight Matrix (CWM), feature selection, and decision tree learning. A general view of the learning component is
illustrated in Fig. 7.
6.1. Episode generation and labeling
At the beginning, we split the training set, T, into two sets
using expert knowledge. The ﬁrst set, T 1 , contains those alerts
that are known to be part of a multi-step attack scenario. The
other set, T 2 , contains either false positive alerts or alerts that
cannot be considered as a critical threat in the network. Using
Algorithm 5.1, we generate episodes with length up to N and
label them manually.
6.2. Correlation Weight Matrix (CWM)
Measuring the correlation strength between attack types plays
an important role in detecting multi-step attacks. Zhu and Ghorbani [6] proposed a Correlation Weight Matrix (CWM) that calculates the correlation strengths of two IDS generated attack types
using a probabilistic alert correlation method. We adopted the Correlation Weight Matrix learning system in [6] and used it in the
learning and detection phase to ﬁlter critical episodes. The CWM
encodes the correlation strength between each pair of alerts that
are known to be part of a multi-step attack scenario and have been
seen so far. By observing new attack strategies, CWM gets updated
accordingly. The formal deﬁnition of CWM is given below.
Deﬁnition. Correlation Weight Matrix (CWM) for n attack types,
at 1 ; at 2 ; . . . ; at n , is a nonsymmetric square matrix of size n, where
cell CWMðat i ; at j Þ contains the correlation weight of attacks at i and
atj .
Fig. 8 shows a sample CWM for alerts at 1 ; at 2 ; at 3 , and at 4 . As
shown in Fig. 8, CWMðat 1 ; at 2 Þ and CWMðat 2 ; at 1 Þ represent two
different temporal relations. CWMðat 1 ; at 2 Þ shows that attack type
at 2 happens after at 1 , while Wðat2 ; at 1 Þ indicates that at 1 happens

6. Learning component
Supervised learning method is used to learn both multi-step attack scenarios and benign episodes. Learning process requires

Fig. 8. Correlation Weight Matrix.

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

1373

after at 2 . By discerning these two situations, one can achieve better
understanding of the relationship between these two combinations of the attack types. Each cell in CWM is computed as follows:

CWMðat i ; at j Þ ¼

N
X
pi;j ðkÞ

ð1Þ

k¼1

N is the number of times these two types of alert have been directly correlated, and p(k) is the probability of the kth correlation.
These correlation weights are incrementally updated during training process.
To normalize the values in the CWM matrix, the following formula is used which encodes CWM values to a number between 0
and 1. The normalized value represents the temporal and causal
relationship between two types of alerts.

CWMðat i ; at j Þ
jCWMðat i ; at j Þj ¼ PN
k¼1 CWMðat i ; at j Þ

ð2Þ

In order to build a CWM, ﬁrst we extract all of the possible episodes in the training set. Then, those episodes whose steps are part
of a multi-step attack are determined using expert knowledge and
are called attack episodes. The attack episodes inserted in the episode tree are used for generating CWM matrix.
6.3. Feature selection
The attributes of episodes are used for learning attack strategies. Labelled episodes are used for extracting features that are
used by the decision tree to learn attack and benign episodes. For
an episode e, with length one, we obtain the following features
to decide about its criticality.
 If esrcIP 2 Asset or edstIP 2 Asset (asset is a set of IPs that belong to
the critical servers in a network).
 If esev erity > h2 (h2 is a threshold for attack type severity).
 If eattackType 2 AttackT (AttackT is a set of all known attack types
that have been seen so far).
 Self-correlation-weight in CWM ðCWMðeattackType ; eattackType ÞÞ.
For an episode with length more than one, we deﬁne following
attributes:
 Average similarity between two source IP addresses of two consecutive episodes
In a network-based attack, the source IP address can be viewed
as the identity of an attacker. In some cases, two alerts with the
same source IP address are likely to belong to the same attack
scenario and therefore could be correlated. However, an
attacker may use different IP addresses to perform different
attacks against the target system or network. Or even worse,
the attackers may spoof their IP address and then attack the target. Therefore, the source IP address cannot always be used to
identify the attacker. The value of this feature indicates the likelihood that two alerts come from the same attacker. The following formula proposed in [6] has been used here to calculate the
similarity between IP addresses:

simðip1 ; ip2 Þ ¼ n=32
In the above formula, n is the number of consecutive high-order bits
that these two IP addresses match, and 32 is the number of bits in
an IP address. For example, if we have:
ip1 = 192.168.0.001 with binary representation 11000000 101010
00 00000000 00000001.
ip2 = 192.168.0.201 with binary representation 11000000
10101000 00000000 10000001,then n = 24 and simðip1 ; ip2 Þ¼ 0:75.

Fig. 9. Hash table for ﬁnding the similarity between two IP sets.

This indicates that these two IP addresses are from the same network and could be used by the same attacker in the different steps
of an attack.
Since we want to measure the similarity between sets of IP addresses, instead of measuring the similarity between two IP addresses, we have to deal with similarity between two sets of IP
addresses. Such a similarity-function is represented by a function
that is named SIM. If we have two sets of IPs, S1 and S2 , at the beginning, we obtain the network address of each IP address in the ﬁrst set
and insert it into a hash table, as is shown in Fig. 9. Next, we calculate
the network address of each IP address in S2 . In this example, they are
192.168.183.0, 201.15.101.0, and 185.20.0.0. At ﬁrst, we ﬁnd each
network address in the hash table, and pick the set of IP addresses
that are associated with that network address. Next, we ﬁnd the
maximum similarity between the IP address of each network address
and the set of IP addresses that are found. For example, in Fig. 9, set S2
contains IP address 192.168.183.11 with network address
192.168.183.0. Since this network address exists in the hash table,
we can ﬁnd the similarity between 192.168.183.11 and the IP addresses in IPs column of 192.168.183.0. That is,

simð192:168:183:5; 192:168:183:11Þ ¼ 28=32 ¼ 87:5%
simð192:168:183:10; 192:168:183:11Þ ¼ 31=32 ¼ 96:87%
Therefore, we pick the maximum one that is 87.5%. Selecting the
maximum similarity of two IP address sets gives a higher weight
to a situation where the same IP address has been used to conduct
second step of a multi-step attack scenario. The, we look up network address 201.15.101.0 in the hash table and ﬁnd

simð201:15:101:2; 201:15:101:220Þ ¼ 24=32 ¼ 75%
Since there is not any other similar network address entry in the
hash table, and the maximum similarity is 96.87%,

SIMðS1 ; S2 Þ ¼ 96:87%
The similarity between the source IP address of an alert with the
source IP address of its subsequent alert is representative of a
one-to-many attack attempt that same attacker (with the same IP
address) launches attacks to one or many other IP addresses. For
an episode with length l, it is computed as shown below:

Pl1

i¼1 SIMðei:srcIP ; eiþ1:srcIP Þ

l1

ð3Þ

 Average similarity between two destination IP addresses of each
two consecutive episodes
Similarity between the destination IP address of an alert with
the destination IP address of its subsequent alert is representative of a many-to-one attack attempt that same target (with

1374

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

same IP address) is exposed to the same or different attacks
from the same or different attackers. It is computed as follows
for an episode e:

Pl1

i¼1 SIMðei:dstIP ; eiþ1:dstIP Þ

l1

ð4Þ

 Average similarity between the destination IP address of an episode
with the source IP address of its consequent episode
Similarity between the destination IP address of an alert with
the source IP address of its consequent alert is representative
of a situation that a destination IP address be compromised,
and consequently same IP starts lunching other attacks to the
other destinations. For an episode e, it is calculated as follow:

Pl1

i¼1 SIMðei:dstIP ; eiþ1:srcIP Þ

l1

ð5Þ

 Average correlation weight of two consequent attack types
The value of this feature is between 0 and 1, indicating the correlation strength between two types of alerts. For example, SadmindPing alerts are usually closely correlated with SadmindBOF
alerts. For deciding about correlation of two alerts, this knowledge is beneﬁcial.

Pl1

i¼1 CWMðei:attackType ; eiþ1:attackType Þ

l1

ð6Þ

 Average attack type severity of each step of an episode
It is a measure for estimating criticality of the attack according
to the severity of the attack type. It is computed as follow for an
episode e.

Pl

i¼1 ei:sev erity

l

ð7Þ

6.4. Decision tree learning
Decision tree learning algorithm is used to acquire speciﬁcation
of different lengths of episodes. We obtain distinctive lengths of
episodes and label them manually accordingly. As is shown in
Fig. 10, at the beginning, features of each set of episodes from
length 1 to 6 (that is the maximum predictable length of a multistep attack scenario) are extracted as is described in Section 6.3.
Afterwards, a separate decision tree is constructed for each length
of training episode set. These decision trees will be used in the
detection component to decide about validity of episodes in the
test set.

Labelled episodes from training set are learned using J48 algorithm, which is the Java version of the C4.5. The C4.5 [24] developed by Ross Quinlan is an algorithm used to generate a
decision. This algorithm is an extension of Quinlan’s earlier ID3
algorithm. It builds decision trees from a set of training data in
the same way as ID3, using the concept of Information Entropy.
7. Detection component
This component is responsible for detecting attack scenarios
among new alert streams. A general view of this component is
shown in Fig. 11. In each batch window, the alert aggregation component aggregates all of the similar alerts. Then, the episode mining component extracts all of the possible episodes with length l,
1 < l 6 N and N ¼ 1 . . . 6. Next, the decision tree that is constructed
in the learning phase decides about each episode. Those episodes
that are detected as malicious episodes will be inserted into the
episode tree, as is discussed in Section 5. These episodes are considered as multi-step attacks, and they will be used for further attack predictions. The detected attack episodes with length l are
used in the generation of episodes with length l þ 1. Therefore,
all of the benign episodes will be excluded from the episode mining process. Episodes that are detected as benign episodes will be
fed into the new strategy detection component to verify whether
or not these episodes are new attack strategies. New attack strategy detection component is discussed in Section 9.
8. Prediction component
A prediction component is responsible for forecasting steps of a
multi-step attack. Attack episodes that are identiﬁed in the detection phase are mined in the model episode tree to predict probable
future episodes that can happen later on.
An attack tree is an episode tree that is constructed in the detection phase based on detected multi-step attacks in the current alert
stream. Hence, episodes that are detected in the detection phase
are inserted into the attack tree. The structure of an attack tree is
exactly the structure of a model episode tree. However, the only
difference between them is their functionality. A model episode
tree is used for keeping information about known multi-step attacks; while, an attack tree is used for keeping information about
detected attacks in the current alert stream.
We deﬁne the frequency of an episode as the fraction of episode
windows in which the episode occurs in a batch window time. That
is, given an alert sequence sbatch in a batch window ðwinbatch Þ and

Fig. 10. Decision tree (DT) construction based on labelled episodes.

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

1375

Fig. 11. Detection component.

episode window width ðwinepisode Þ, the frequency of an episode e in
winbatch is

frðe; sbatch ; winepisode Þ ¼

jw 2 Wðsbatch ; winepisode Þje occurs in wj
jWðsbatch ; winepisode Þj

ð8Þ

Once the attack episodes are created, they can be used to obtain
rules that describe connections between alerts in the given alert sequence. For example, if we know that the episode e2 of Fig. 3 occurs
and that the super-episode e3 exists in the model episode tree, we
can estimate that after seeing a window with A and B, there is a
chance that C occurs after A and B. Formally, an episode rule is an
expression e2 ) e3 ðconfidenceÞ, where e2 and e3 are episodes such
that e2  e3 while conﬁdence of the episode rule is 1  b
d dev ðe2 ; e3 Þ.
Actually, by observing each new episode in the attack tree, we calculate the frequency of each sub-episode of it and then apply a deviation distance method, ð b
d dev ðe2 ; e3 ÞÞ, to ﬁnd the similarity between
the episode that is seen in the model episode tree and the one that
is inserted into the attack tree. This similarity is our degree of conﬁdence about the predicted rule.
For example, if e3 is an episode in the model episode tree with n
steps, we represent the frequency of each step as freqe1 ; freqe2
   freqen . Therefore, if episode e2 with length l is seen in the attack
tree and episode e3 with length l + 1 is a super-episode of episode
e2 in the model episode tree, we represent the frequency of steps of
episode e2 as a vector S and the frequency of steps of episode e3 as a
vector T. Then, using Eq. (9), we calculate the deviation distance,
ddev between sets S and T which have n elements. Next, we compute the normalized deviation distance, b
d dev , using Eq. (10). Moreover, 1  b
d dev ðe2 ; e3 Þ shows our conﬁdence about a rule. By
computing a conﬁdence for each step of episode, we represent
the likelihood percentage between the current alert stream and
the learned alert set.

ddev ðS; TÞ ¼

n
X

ji  jj where TðjÞ ¼ SðiÞ ¼ k

According to Fig. 12, the episode A is seen 30 times in 45 episode windows of the batch window and the episode AB is seen

Fig. 12. Number of occurrences of episodes in the model episode tree ET.

ð9Þ

k¼1

b
d dev ðS; TÞ ¼

(

2
d ðS; TÞ;
n2 dev
2
d ðS; TÞ;
n2 1 dev

confidence ¼ 1  b
d dev ðe2 ; e3 Þ

if n is even
if nis odd

ð10Þ
ð11Þ

The episode rules show the connections between alerts more
clearly than the episodes alone. For example, the model episode
tree ET ¼ ðE; ; FÞ that is shown in Fig. 6(c) consists of episodes
e1 ; e2 ; e3 , and e4 . The number of occurrences of episodes in a batch
window is shown in Fig. 12.

Fig. 13. Attack tree construction process.

1376

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

that episode will be kept in the tree. If s2  s1 > h, then at ﬁrst that
episode will be frozen once. That means, it will not participate in any
rule prediction, although we keep it in the tree. If that episode is not
updated for second time, it will be pruned from the episode tree and
it will be considered as an old attack strategy that no longer exists in
the network.
Based on our observations in the training alert set, we calculate
the amount of time that elapses between each two pairs of a multistep attack. Therefore, we provide a database that contains h values
for each two consequent attack types in the multi-step attack
scenarios.
9. New attack strategy detection component
Fig. 14. Run time of episode mining algorithm in different episode windows.

10 times in 15 episode windows of the batch window and so forth.
Episode extraction from current alert stream and attack tree construction is illustrated in Fig. 13.
At the beginning, as is shown in Fig. 13(a), we detect an episode
that has attack A with 10 occurrences in 10 episode windows of the
batch window and insert it into the attack tree. Consequently, we
seek this episode in the model episode tree and ﬁnd attacks B and
D that can occur after A. Therefore, we conclude episode rules
A ) Bð67%Þ and A ) Dð50%Þ that are shown in Fig. 13(a). Since
the number of occurrences of A is 10 and it happened during 10
episode windows of the batch window in the attack tree, frequency
of A is (10/10). An episode that has attacks A and B is seen 10 times
during 15 episode windows of the batch window in the model episode tree, while another episode that has attacks A and D is seen 20
times during 20 episode windows of the
30batch
 window in
 the20mod
el episode tree. We use vectors S1 ¼ 45
; 10
;
and S2 ¼ 30
for
15
45 20
episodes
10 AB and AD respectively. Moreover, we use vector
T ¼ 10 ; 0 for the episode A. Since normalized distance deviation
of two vectors S1 and T is 0.5, and normalized distance deviation
between two vectors S2 and T is 0.67, we can draw two conclusions: First, up to (1  0.5 = 0.5) 50% we are conﬁdent that B happens after A. Next, up to (1  0.67 = 0.33) 33% we are certain that D
occurs after A.
Now we suppose that episode AD happens 5 times in 10 episode
windows of the batch window and we insert it into the attack tree
while episode A happened 15 times in 15 episode windows of the
batch window. Hence, as is shown in Fig. 13(b), we predict that
 20 10
either attack F or G happens after AD. Vectors S1 ¼ 30
; ;
and
45 20 25
30 20 10
S2 ¼ 45 ; 20 ; 30 represent frequencies for probable attacks ADF


; 5 ; 0 shows frequency
and ADG respectively, while vector T ¼ 15
15 10
of episode AD. Normalized distance deviation of two vectors S1 and
T is 0.31 and normalized distance deviation of two vectors S2 and T
is 0.29. So, we infer rules AD ) Fð69%Þ and AD ) Gð71%Þ.
In Fig. 13(c), it is assumed that after attack AD, attack F happens,
and ADF is reported as a complete multi-step attack. Therefore, we
predict with 93% conﬁdence that G can occur after AD.
In Fig. 13(b), episode AB is seen 5 times; and so, by searching
into the model episode tree, rule AB ! C is inferred with 50% conﬁdence ((5/10)  100 = 50%). By detecting episode ABC in Fig. 13(c),
the system triggers an alarm and informs the administrator about a
detected multi-step attack. However, when episode AD is seen, the
rule A ! D is predicted with 60% conﬁdence.
In each batch window, after detecting attack episodes, these episodes will be inserted into the attack tree. At the same time, a timer
that shows the time that an episode is inserted into the episode tree
s will be set. We consider a time duration threshold h for the duration that one branch of an episode tree will be kept in the tree for
rule prediction. In other words, if an episode is inserted into the episode tree in time s1 and it is updated in the time s2 and s2  s1 < h,

When the detection component distinguishes between attack
and benign episodes, the benign episodes that are either a false positive or a new multi-step attack scenario are separated. We apply a
rule-based classiﬁcation method to determine whether or not an
episode is a new attack strategy. When the administrator conﬁrmed
the strategy, the attributes of detected new attack strategy will be
included in the CWM and decision tree. That is, we consider the newly detected attack strategies for further multi-step attack detections.
The rule-based classiﬁcation component in the framework relies on the attributes of alerts. For a set of episodes E, each episode
e has the following attributes: source IP address, destination IP address, source and destination port number, attack type of alert, attack severity, time occurrence and frequency that are represented
as esrcIP ; edstIP ; esrcPort ; edstPort ; eattackType ; esev erity ; etime , and efr , respectively. For the episodes with length one, we apply the following criteria to decide about criticality of a new episode.
 If efr > h1 (h1 is a threshold for frequency).
 If esrcIP 2 Asset or edstIP 2 Asset (Asset is a set of IPs that belong to
critical servers in the network).
 If esev erity > h2 (h2 is a threshold for attack type severity).
For an episode e that is shown as e1 ; e1 ; . . . ; el , with length more
than one, we consider the following criteria to decide about it.
 If 9i 2 f1; . . . ; lg such that ei:attackType : 2 AttackT (AttackT is a set
of all attack types that have been seen so far)
Pl1 Pl
SIMðei:srcIP ;eiþ1:srcIP Þ
 If i¼1 i¼2lðl1Þ=2
> h3 (h3 is an IP similarity threshold)
Pl1
SIMðei:dstIP ;eiþ1:srcIP Þ
i¼1
 If
> h3
Pl1 Pl l1
SIMðei:dstIP ;eiþ1:dstIP Þ
i¼1
i¼2
 If
> h3
lðl1Þ=2
Pl
e
i¼1 i:sev erity
 If
> h4 (h4 is a severity threshold)
l
In other words, a new attack strategy probably contains new attack types, and there is similarity between IP addresses of each two
pairs of its episode, and average severity of its steps exceeds a
threshold. We considered three possibilities for IP address
similarity:
 similarity between the source IP address of each alert with the
source IP address of the other subsequent alerts
 similarity between the destination IP address of an alert with
the source IP address of its consequent alert
 similarity between the destination IP address of each alert with
the destination IP address of the other consequent alerts
10. Evaluation
In this section, we show the results of experiments performed
for evaluating the proposed framework. The main purpose of evaluation is demonstrating the effectiveness and robustness of our

1377

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379
Table 1
CWM for LLDOS1.0.

Sadmind Ping
Admin
Sadmind Amslverify
Overﬂow
Rsh
Mstream Zombie
Stream DoS

Sadmind
Ping

Admin

Sadmind
Amslverify
Overﬂow

Rsh

Mstream
Zombie

Stream
DoS

1
0.01
0.01

0.3
1
0.01

0.99
0.3
1

0.41
0.81
0.99

0.81
0.99
0.3

0.21
0.41
0.81

0.01
0.01
0.01

0.01
0.01
0.01

0.01
0.01
0.01

1
0.8
0.01

0.01
1
0.01

0.99
0.99
1

approach. We use LLDOS 1.0 and MS-SQL Server Worm to examine
the correctness of our developed approach. CMW matrixes that are
constructed in the learning phase are presented for each episode
tree. Besides, we evaluate the accuracy of the decision tree. At
the end, we obtain run time of the algorithm in different lengths
of episode windows. The experimental results are obtained by running the code on a computer with 2 Gb of RAM, Intel Core2
2.20 GHz CPU, and Windows XP operating system.
10.1. Evaluation on LLDOS 1.0
DARPA 2000 [22] is a well-known IDS evaluation dataset created
by MIT Lincoln Laboratory. Even after twelve years of its generation,
DARPA dataset is still rigorously used by the research community
for evaluation of IDSs. The dataset, even though old, still carries a
lot of novelty and sophistication in attacks [25]. As alert correlation
and not IDS evaluation is the main focus of this study, the IDS alert
set generated from the DARPA network trafﬁc is still valuable for
discovering multi-step attack scenarios. In this study, the alerts
are generated by Snort IDS after replaying the DARPA network trafﬁc. DARPA 2000 consists of two multi-step attack scenarios, namely
LLDOS1.0 and LLDOS2.0. Both attack scenarios include a Distributed
Denial of Service (DDoS) attack, with different stealth levels. In this
paper we only show the results of evaluation on LLDOS1.0. To this
end, we ﬁnd different combination of alerts by extracting episodes.
Then, we verify whether or not an episode is similar to a multi-step
attack scenario using decision tree learning method. Here we applied our approach on LLDOS1.0 alert set to show the resulting
CWM matrix which is shown in Table 1. We learn correlation
weight between alert types to decide about episodes.
Table 2 shows frequency of each attack episode in LLDOS1.0 data
set. For example, in 5 min episode window and one hour batch window, frequency of Rsh was 0.08. Frequency of each episode will be
kept in episode tree to be used in the episode rule prediction step.
Episode generation algorithm that is explained in Section 3 creates a large number of episodes. In this work, we ﬁlter the most
important episodes that are similar to known multi-step attacks.
After detecting and ﬁltering critical episodes, number of episodes
reduces dramatically. Reduction in the number of episodes is shown
in Table 3. Mostly episodes are reduced more that 90% in each step.
10.2. Evaluation on worm propagation
We gathered real network trafﬁc during a worm propagation
from an ISP to evaluate the accuracy of our algorithm. The corresponding alerts are obtained via Snort IDS. The worm propagation
scenario has four steps. In the ﬁrst step, worm sends ICMP pings to
a list of IP addresses in the network to recognize the live ones.
Second, after detecting live servers, it does UDP port 1434 sweeping to detect vulnerable servers. Third, Worm tries to execute the
buffer overﬂow on the candidate servers. Finally, the worm propagates itself all over the network. In Table 4, you can ﬁnd correlation
weight of attack types for each two steps of worm scenario. Since a

Table 2
Frequency of attack episodes in 5 min episode window and one
hour batch window in LLDOS1.0.
Attack type

Frequency

Sadmind Ping
Admin
Sadmind Amslverify Overﬂow
Rsh
Mstream Zombie
Stream DoS

0.16
0.16
0.16
0.08
0.08
0.08

Table 3
Episode reduction in LLDOS1.0.

Episode
Episode
Episode
Episode
Episode

length
length
length
length
length

1
2
3
4
5

No. of episodes
without reduction

No. of episodes
with reduction

Reduction
percentage
(%)

19
171
969
3876
11,628

6
15
20
15
6

68.42
91.22
97.93
99.61
99.94

Table 4
CWM for worm propagation.

MSSQL worm
propagation
attempt
MS-SQL version
overﬂow
attempt
ICMP Ping
(portscan) UDP
portsweep

MSSQL Worm
propagation
attempt

MS-SQL version
overﬂow
attempt

ICMP
Ping

(Portscan)
UDP
portsweep

1

0.81

0.41

0.21

0.01

1

0.81

0.41

0.01
0.01

0.01
0.01

1
0.01

0.81
1

Table 5
Frequency of attack episodes in 5 min episode window and 10 min batch window in
worm propagation.
Attack type

Frequency

MSSQL worm propagation attempt
MS-SQL version overﬂow attempt
ICMP Ping
(Portscan) UDP portsweep

2.2
2.2
8.66
5

worm can propagate in a couple of seconds, frequency of each step
of worm propagation attack is higher in the same window width.
Table 5 shows that frequency of each step of worm scenario happened in 5 min episode window and one hour batch window. In

1378

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379

Table 6
Episode reduction in worm propagation.

Episode
Episode
Episode
Episode

length
length
length
length

1
2
3
4

No. of episodes
without reduction

No. of episodes
with reduction

Reduction
percentage
(%)

10
45
120
210

4
6
4
1

60
86.66
96.66
99.52

the worm propagation scenario, we had a considerable reduction
in the number of generated episodes too. Table 6 demonstrates
reduction percentage in each step.
10.3. Decision tree evaluation
We applied J48 algorithm to learn about speciﬁcation of different lengths of episodes. Learned decision trees are employed for
deciding about episodes in unseen data sets. Correspondingly,
accuracy of decisions in decision tree plays an important role in
detecting critical multi-step attacks.
For decision tree training and testing purpose, we used two data
sets: LLDOS1.0 and worm propagation data set. First, each data set
is divided into two sets called training set and test set. Then, for
each training data set, different lengths of episodes are generated,
labelled and their attributes are extracted to train the decision tree.
Next, the test set which contains the other half of each data set is
given to the decision tree to evaluate its detection accuracy.
Tables 7–9 show accuracy of decision trees for episodes with
length one, two and six on test set using ﬁve times cross validation.
10.4. Evaluation of new attack strategy detection
New attack strategy detection method is discussed in Section 9.
We discover new attack strategies based on a rule-based classiﬁca-

Table 7
Accuracy of classiﬁcation of decision trees on episodes with length one.

Test set of
LLDOS1.0
Test set of worm
propagation

Correctly classiﬁed
instances (length 1) (%)

Incorrectly classiﬁed
instances (length 1) (%)

95.33

4.66

99.78

0.21

Table 8
Accuracy of classiﬁcation of decision trees on episodes with length two.

Test set of
LLDOS1.0
Test set of worm
propagation

Correctly classiﬁed
instances (length 2) (%)

Incorrectly classiﬁed
instances (length 2) (%)

94.61

5.39

98.92

1.08

Table 9
Accuracy of classiﬁcation of decision trees on episodes with length six.

Test set of
LLDOS1.0

Correctly classiﬁed
instances (length 6) (%)

Incorrectly classiﬁed
instances (length 6) (%)

98.73

1027

tion method and update our knowledge-base accordingly. To evaluate accuracy of our method, we added worm propagation alert set
to the end of LLDOS1.0 alert set and ran our algorithm. After testing
new data set, our method detected new attack strategy and
reported it as a new multi-step attack. After conﬁrmation of detected attack, decision trees and CWM got updated accordingly.
10.5. Run time evaluation
One of the most time-consuming parts of our method is generating possible episodes in an alert stream. We measured run time
of our algorithm using different episode windows to prove effectiveness of our approach. According to the results, shown in
Fig. 14, the run time of the algorithm increases as we enlarge the
width of episode window. For two hours episode window, our
method needed 218, 438 ms to run, that is acceptable. The main
reason of obtaining a good performance in episode generation process is omitting meaningless episodes during each step. This
means, at the end of the process, a smaller number of critical
episodes is generated. The overall run time of the algorithm is
one second for two hours episode window and 24 h batch window.
11. Conclusion and future works
In this paper we proposed a multi-layer framework for detecting critical single-step and multi-step attack scenarios that alleviates the burden of looking through a huge number of alerts, of
which most are false positive. At the beginning, we aggregated
alerts based on similarity between their attack types. Next, we applied an episode mining algorithm to discover possible combination of alerts. Through a supervised Decision Tree (DT) learning
method, we gained knowledge about critical alert patterns that
represent multi-step attacks. We also provided a prediction facility
in the framework to forecast further steps of a multi-step attack by
observing its early behaviours.
In this paper, we tried a limited number of features for learning
patterns and just one machine learning method for acquiring
knowledge about episodes. Moreover, we restricted our expert
knowledge to a limited source of information about network proﬁle. One of the possible extensions for this work is examining an
extensive range of possible features for attack episode learning
component.
The alert correlation system that is developed in this thesis can
be used for real-time correlation and notiﬁcation purposes. However, a visualization mechanism that undertakes a user interface
needs to be developed for fulﬁlling this demand.
Although the results show very good multi-step attack detection rates, the proposed method needs to be evaluated using more
diverse data sets to completely evaluate the impact of episode
reduction on the detection rates. Also, in this research work the impact of pre-deﬁned experimental thresholds on the effectiveness of
the proposed approach has not been analyzed.
References
[1] F. Cuppens, R. Ortalo, Lambda: a language to model a database for detection of
attacks, in: H. Debar, L. Me, S.F. Wu (Eds.), Proceedings of Recent Advances in
Intrusion Detection, 3rd International Symposium, (RAID 2000), Toulouse,
France, October 2000, Lecture Notes in Computer Science, Springer-Verlag,
Heidelberg, 1907, pp. 197–216.
[2] S. Eckmann, G. Vigna, R. Kemmerer, Statl: an attack language for state-based
intrusion detection, Journal of Computer Security 10 (2002) 71–103.
[3] E. Totel, B. Vivinis, L. Mé, A language driven ids for event and alert correlation,
in: SEC, 2004, pp. 209–224.
[4] H. Debar, A. Wespi, Aggregation and correlation of intrusion-detection alerts,
in: Recent Advances in Intrusion Detection, 2001, pp. 85–103.
[5] B. Morin, H. Debar, Correlation of intrusion symptoms: an application of
chronicles,in: RAID, 2003, pp. 94–112.

M. Soleimani, A.A. Ghorbani / Computer Communications 35 (2012) 1368–1379
[6] B. Zhu, A.A. Ghorbani, Alert correlation for extracting attack strategies,
International Journal of Network Security 3 (2) (2006) 244–258.
[7] R.P. Goldman, W. Heimerdinger, S.A. Harp, C.W. Geib, V. Thomas, R.L. Carter,
Information modeling for intrusion report aggregation, in: DARPA Information
Survivability Conference and Exposition (DISCEX II), 2001, pp. 329–342.
[8] P. Ning, D. Xu, C. Healey, R. St. Amant, Building attack scenarios through
integration of complementary alert correlation methods, in: Proceedings of the
11th Annual Network and Distributed System Security Symposium, February
2004.
[9] H. Debar, A. Wespi, The intrusion-detection console correlation mechanism, in:
4th International Symposium on Recent Advances in Intrusion Detection
(RAID), 2001.
[10] K. Levitt, S.J. Templeton, A requires/provides model for computer attacks, in:
Proceedings of the 2000 Workshop on New Security Paradigms, February
2001, pp. 31–38.
[11] R. Sadoddin, A.A. Ghorbani, An incremental frequent structure mining framework
for real-time alert correlation, Computers & Security (2009) 153–173.
[12] D. Li, A. Zhang, Z. Li, L. Wang, Discovering novel multistage attack patterns in
alert streams, in: International Conference on Networking Architecture and
Storage (NAS 2007), Elsevier, North-Holland, 2007, pp. 115–121.
[13] H. Toivonen, H. Mannila, Discovery of frequent episodes in event sequences,
Data Mining and Knowledge Discovery (1997) 259–289.
[14] M. Klemettinen, H. Mannila, H. Toivonen, Interactive exploration of interesting
ﬁndings in the telecommunication network alarm sequence analyzer (tasa),
Information and Software Technology 41 (9) (1999) 557–567.
[15] S. Noel, S. Jajodia, Optimal ids sensor placement and alert prioritization using
attack graphs, Journal of Network and Systems Management 16 (3) (2008)
259–275.

1379

[16] R. Smith, N. Japkowicz, M. Dondo, P. Mason, Using unsupervised learning for
network alert correlation, Lecture Notes in Computer Science, vol. 5032,
Springer, 2008. p. 308.
[17] J. Viinikka, H. Debar, L. Mé, A. Lehikoinen, M. Tarvainen, Processing intrusion
detection alert aggregates with time series modeling, Information Fusion 10
(2009) 312–324.
[18] M.S. Shin, K.J. Jeong, An alert data mining framework for network-based
intrusion detection system, Information Security Applications (2006) 38–53.
[19] L. WANG, A. LIU, S. JAJODIA, Using attack graphs for correlating hypothesizing
and predicting intrusion alerts, Computer Communications (2006) 2917–
2933.
[20] R. Smith, N. Japkowicz1, M. Dondo2, P. Mason, Using unsupervised learning
for network alert correlation, Advances in Artiﬁcial Intelligence (2008) 308–
319.
[21] K. Julisch, Clustering intrusion detection alarms to support root cause analysis,
ACM Transactions on Information and System Security (TISSEC) 6 (4) (2003)
443–471.
[22] MIT Lincoln Laboratory, 2000 darpa intrusion detection scenario speciﬁc data
sets.
<http://www.ll.mit.edu/IST/ideval/data/2000/2000_data_index.html>,
2000 (accessed 09.07.07).
[23] P. Ning, D. Xu, Learning attack strategies from intrusion alert, in: Proceedings
of the ACM Conference on Computer and Communications Security (CCS 03),
October 2003.
[24] Ross Quinlan, C4.5: Program for machine learning, Morgan Kaufmann, 1992.
[25] C. Thomas, N. Balakrishnan, Usefulness of DARPA data set in intrusion
detection system evaluation, in: Proceedings SPIE International Defense and
Security Symposium 2008, October 2008.

